#!/usr/bin/env python3
"""
Audio VAD (Voice Activity Detection) CLI Tool - Version Am√©lior√©e
Analyse les fichiers audio pour d√©tecter UNIQUEMENT la parole claire (pas les murmures/chuchotements).

Installation comme commande syst√®me:
    chmod +x audio-vad
    sudo cp audio-vad /usr/local/bin/
    # ou
    ln -s $(pwd)/audio-vad ~/.local/bin/audio-vad

Usage:
    audio-vad fichier.mp3
    audio-vad fichier.wav --format text --pretty --strict
"""
import torch
import torchaudio
import numpy as np
import json
import argparse
import sys
import os
from datetime import timedelta
from pathlib import Path
import librosa
from scipy import signal
from sklearn.preprocessing import StandardScaler

class AudioVADAnalyzer:
    def __init__(self):
        self.model = None
        self.utils = None
        self._load_model()
    
    def _load_model(self):
        """Charge le mod√®le Silero VAD"""
        try:
            self.model, self.utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )
        except Exception as e:
            raise RuntimeError(f"Impossible de charger le mod√®le Silero VAD: {e}")
    
    @staticmethod
    def format_time(seconds):
        """Convertit les secondes en format HH:MM:SS.mmm"""
        td = timedelta(seconds=seconds)
        total_seconds = int(td.total_seconds())
        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        milliseconds = int((td.total_seconds() - total_seconds) * 1000)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"
    
    def _extract_audio_features(self, wav_segment, sr=16000):
        """
        Extrait des caract√©ristiques audio pour diff√©rencier parole claire des murmures
        
        Returns:
            dict: Caract√©ristiques audio calcul√©es
        """
        # Conversion en float32 si n√©cessaire
        if wav_segment.dtype != np.float32:
            wav_segment = wav_segment.astype(np.float32)
        
        # 1. √ânergie RMS (Root Mean Square)
        rms_energy = np.sqrt(np.mean(wav_segment ** 2))
        
        # 2. √ânergie spectrale dans diff√©rentes bandes
        # Calcul du spectrogramme
        f, t, Sxx = signal.spectrogram(wav_segment, fs=sr, window='hamming', 
                                      nperseg=512, noverlap=256)
        
        # Bandes de fr√©quences importantes pour la parole
        # Basse fr√©quence (80-300 Hz) - fondamentales vocales
        low_freq_mask = (f >= 80) & (f <= 300)
        low_freq_energy = np.mean(Sxx[low_freq_mask, :])
        
        # Moyenne fr√©quence (300-2000 Hz) - voyelles et consonnes
        mid_freq_mask = (f >= 300) & (f <= 2000)
        mid_freq_energy = np.mean(Sxx[mid_freq_mask, :])
        
        # Haute fr√©quence (2000-8000 Hz) - consonnes fricatives
        high_freq_mask = (f >= 2000) & (f <= 8000)
        high_freq_energy = np.mean(Sxx[high_freq_mask, :])
        
        # 3. Ratio des √©nergies (caract√©ristique importante)
        # Parole claire : plus d'√©nergie en moyenne fr√©quence
        # Murmures : √©nergie plus r√©partie ou dominante en basse fr√©quence
        mid_to_low_ratio = mid_freq_energy / (low_freq_energy + 1e-10)
        high_to_low_ratio = high_freq_energy / (low_freq_energy + 1e-10)
        
        # 4. Variabilit√© spectrale (parole claire plus variable)
        spectral_variance = np.var(np.mean(Sxx, axis=0))
        
        # 5. Centro√Øde spectral (fr√©quence "moyenne" du son)
        spectral_centroid = np.sum(f.reshape(-1, 1) * Sxx) / (np.sum(Sxx) + 1e-10)
        
        # 6. Zero Crossing Rate (taux de passages par z√©ro)
        # Parole claire a g√©n√©ralement plus de variations
        zcr = np.sum(np.diff(np.sign(wav_segment)) != 0) / len(wav_segment)
        
        return {
            'rms_energy': float(rms_energy),
            'low_freq_energy': float(low_freq_energy),
            'mid_freq_energy': float(mid_freq_energy),
            'high_freq_energy': float(high_freq_energy),
            'mid_to_low_ratio': float(mid_to_low_ratio),
            'high_to_low_ratio': float(high_to_low_ratio),
            'spectral_variance': float(spectral_variance),
            'spectral_centroid': float(spectral_centroid),
            'zero_crossing_rate': float(zcr)
        }
    
    def _is_clear_speech(self, features, strict_mode=False):
        """
        D√©termine si un segment contient de la parole claire bas√© sur les caract√©ristiques
        
        Args:
            features: Dictionnaire des caract√©ristiques extraites
            strict_mode: Mode strict pour une d√©tection plus s√©lective
            
        Returns:
            tuple: (is_clear_speech, confidence_score, reasons)
        """
        reasons = []
        score = 0
        max_score = 0
        
        # Seuils adaptatifs selon le mode
        if strict_mode:
            # Mode strict : crit√®res plus s√©v√®res
            rms_threshold = 0.02  # √ânergie minimum plus √©lev√©e
            mid_low_ratio_min = 1.5  # Ratio plus √©lev√©
            zcr_min = 0.05  # Plus de variations requises
            spectral_variance_min = 1e-6
        else:
            # Mode normal
            rms_threshold = 0.01
            mid_low_ratio_min = 1.0
            zcr_min = 0.03
            spectral_variance_min = 1e-7
        
        # Crit√®re 1: √ânergie RMS suffisante
        max_score += 20
        if features['rms_energy'] > rms_threshold:
            score += 20
            reasons.append(f"‚úì √ânergie suffisante ({features['rms_energy']:.4f})")
        else:
            reasons.append(f"‚úó √ânergie trop faible ({features['rms_energy']:.4f})")
        
        # Crit√®re 2: Ratio moyenne/basse fr√©quence (crucial pour diff√©rencier parole claire)
        max_score += 25
        if features['mid_to_low_ratio'] > mid_low_ratio_min:
            score += 25
            reasons.append(f"‚úì Bon ratio mid/low freq ({features['mid_to_low_ratio']:.2f})")
        else:
            reasons.append(f"‚úó Ratio mid/low freq faible ({features['mid_to_low_ratio']:.2f})")
        
        # Crit√®re 3: Variabilit√© spectrale (parole vs bruit constant)
        max_score += 20
        if features['spectral_variance'] > spectral_variance_min:
            score += 20
            reasons.append(f"‚úì Variabilit√© spectrale pr√©sente ({features['spectral_variance']:.2e})")
        else:
            reasons.append(f"‚úó Manque de variabilit√© spectrale ({features['spectral_variance']:.2e})")
        
        # Crit√®re 4: Zero Crossing Rate (articulation)
        max_score += 15
        if features['zero_crossing_rate'] > zcr_min:
            score += 15
            reasons.append(f"‚úì Bonne articulation (ZCR: {features['zero_crossing_rate']:.4f})")
        else:
            reasons.append(f"‚úó Articulation faible (ZCR: {features['zero_crossing_rate']:.4f})")
        
        # Crit√®re 5: Centro√Øde spectral dans la gamme vocale
        max_score += 10
        if 200 <= features['spectral_centroid'] <= 3000:
            score += 10
            reasons.append(f"‚úì Centro√Øde dans gamme vocale ({features['spectral_centroid']:.0f} Hz)")
        else:
            reasons.append(f"‚úó Centro√Øde hors gamme vocale ({features['spectral_centroid']:.0f} Hz)")
        
        # Crit√®re 6: √ânergie dans les hautes fr√©quences (consonnes)
        max_score += 10
        if features['high_to_low_ratio'] > 0.1:
            score += 10
            reasons.append(f"‚úì Pr√©sence de consonnes (ratio high/low: {features['high_to_low_ratio']:.3f})")
        else:
            reasons.append(f"‚úó Manque de consonnes (ratio high/low: {features['high_to_low_ratio']:.3f})")
        
        confidence = (score / max_score) * 100
        
        # Seuil de d√©cision
        threshold = 70 if strict_mode else 60
        is_clear = confidence >= threshold
        
        return is_clear, confidence, reasons
    
    def analyze_audio(self, audio_path, config=None, strict_mode=False):
        """
        Analyse un fichier audio pour d√©tecter UNIQUEMENT la parole claire
        
        Args:
            audio_path: Chemin vers le fichier audio
            config: Configuration pour l'analyse VAD
            strict_mode: Mode strict pour filtrer plus s√©v√®rement
        
        Returns:
            dict: R√©sultats de l'analyse
        """
        if config is None:
            # Configuration plus stricte par d√©faut
            config = {
                'threshold': 0.6 if strict_mode else 0.5,  # Seuil VAD plus √©lev√©
                'min_speech_duration_ms': 500 if strict_mode else 250,  # Dur√©e minimum plus longue
                'min_silence_duration_ms': 200,
                'window_size_samples': 1536,
                'speech_pad_ms': 50  # Padding l√©g√®rement augment√©
            }
        
        # V√©rifier l'existence du fichier
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Fichier audio introuvable: {audio_path}")
        
        (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = self.utils
        
        # Lire le fichier audio
        try:
            wav = read_audio(audio_path, sampling_rate=16000)
        except Exception as e:
            raise RuntimeError(f"Erreur lors de la lecture du fichier audio: {e}")
        
        # D√©tecter les timestamps de parole avec Silero VAD
        speech_timestamps = get_speech_timestamps(
            wav, 
            self.model, 
            sampling_rate=16000,
            **config
        )
        
        # Calculer la dur√©e totale
        total_duration = len(wav) / 16000
        
        # Filtrer les segments avec analyse acoustique avanc√©e
        filtered_segments = []
        segment_details = []
        
        for i, segment in enumerate(speech_timestamps):
            start_sample = segment['start']
            end_sample = segment['end']
            
            # Extraire le segment audio
            wav_segment = wav[start_sample:end_sample].numpy()
            
            # Analyser les caract√©ristiques acoustiques
            features = self._extract_audio_features(wav_segment)
            
            # D√©terminer si c'est de la parole claire
            is_clear, confidence, reasons = self._is_clear_speech(features, strict_mode)
            
            start_time = start_sample / 16000
            end_time = end_sample / 16000
            duration = end_time - start_time
            
            segment_detail = {
                'original_id': i + 1,
                'start_seconds': start_time,
                'end_seconds': end_time,
                'duration_seconds': duration,
                'start_formatted': self.format_time(start_time),
                'end_formatted': self.format_time(end_time),
                'duration_formatted': self.format_time(duration),
                'is_clear_speech': is_clear,
                'confidence_score': confidence,
                'acoustic_features': features,
                'analysis_reasons': reasons
            }
            
            segment_details.append(segment_detail)
            
            # Ne garder que les segments de parole claire
            if is_clear:
                filtered_segments.append({
                    'id': len(filtered_segments) + 1,
                    'type': 'CLEAR_SPEECH',
                    'start_seconds': start_time,
                    'end_seconds': end_time,
                    'duration_seconds': duration,
                    'start_formatted': self.format_time(start_time),
                    'end_formatted': self.format_time(end_time),
                    'duration_formatted': self.format_time(duration),
                    'confidence_score': confidence
                })
        
        # Calculer les statistiques
        stats = self._calculate_statistics(filtered_segments, segment_details, total_duration, strict_mode)
        
        # Pr√©parer le r√©sultat
        result = {
            'file_info': {
                'path': os.path.abspath(audio_path),
                'filename': os.path.basename(audio_path),
                'total_duration_seconds': total_duration,
                'total_duration_formatted': self.format_time(total_duration)
            },
            'analysis_config': {
                **config,
                'strict_mode': strict_mode,
                'acoustic_analysis_enabled': True
            },
            'statistics': stats,
            'clear_speech_segments': filtered_segments,
            'all_detected_segments': segment_details  # Pour debug/analyse
        }
        
        return result
    
    def _calculate_statistics(self, clear_segments, all_segments, total_duration, strict_mode):
        """Calcule les statistiques de l'analyse am√©lior√©e"""
        total_clear_speech = sum(s['duration_seconds'] for s in clear_segments)
        total_detected = sum(s['duration_seconds'] for s in all_segments)
        
        # Segments rejet√©s (probablement murmures/bruit)
        rejected_segments = [s for s in all_segments if not s['is_clear_speech']]
        total_rejected = sum(s['duration_seconds'] for s in rejected_segments)
        
        # Moyennes des scores de confiance
        if clear_segments:
            avg_confidence = np.mean([s['confidence_score'] for s in clear_segments])
        else:
            avg_confidence = 0
        
        if all_segments:
            avg_confidence_all = np.mean([s['confidence_score'] for s in all_segments])
        else:
            avg_confidence_all = 0
        
        stats = {
            'analysis_mode': 'strict' if strict_mode else 'normal',
            'total_segments_detected': len(all_segments),
            'clear_speech_segments': len(clear_segments),
            'rejected_segments': len(rejected_segments),
            'total_clear_speech_seconds': total_clear_speech,
            'total_detected_speech_seconds': total_detected,
            'total_rejected_seconds': total_rejected,
            'total_silence_seconds': total_duration - total_detected,
            'total_clear_speech_formatted': self.format_time(total_clear_speech),
            'total_detected_speech_formatted': self.format_time(total_detected),
            'total_rejected_formatted': self.format_time(total_rejected),
            'clear_speech_percentage': (total_clear_speech / total_duration * 100) if total_duration > 0 else 0,
            'detected_speech_percentage': (total_detected / total_duration * 100) if total_duration > 0 else 0,
            'rejection_rate': (total_rejected / total_detected * 100) if total_detected > 0 else 0,
            'average_confidence_clear': avg_confidence,
            'average_confidence_all': avg_confidence_all,
            'quality_metrics': {
                'segments_with_high_confidence': len([s for s in clear_segments if s['confidence_score'] >= 80]),
                'segments_with_medium_confidence': len([s for s in clear_segments if 60 <= s['confidence_score'] < 80]),
                'segments_with_low_confidence': len([s for s in clear_segments if s['confidence_score'] < 60])
            }
        }
        
        return stats

def create_enhanced_text_report(analysis_result, output_path):
    """Cr√©e un rapport texte d√©taill√© avec l'analyse acoustique"""
    content = []
    
    # En-t√™te
    content.append("=== ANALYSE AUDIO AVANC√âE - PAROLE CLAIRE UNIQUEMENT ===\n")
    content.append(f"Fichier: {analysis_result['file_info']['filename']}")
    content.append(f"Dur√©e totale: {analysis_result['file_info']['total_duration_formatted']}")
    content.append(f"Mode d'analyse: {analysis_result['statistics']['analysis_mode'].upper()}")
    content.append("\n" + "="*70 + "\n")
    
    # Statistiques d√©taill√©es
    stats = analysis_result['statistics']
    content.append("STATISTIQUES D√âTAILL√âES:")
    content.append(f"Segments d√©tect√©s par VAD: {stats['total_segments_detected']}")
    content.append(f"Segments de parole claire: {stats['clear_speech_segments']}")
    content.append(f"Segments rejet√©s (murmures/bruit): {stats['rejected_segments']}")
    content.append(f"Taux de rejet: {stats['rejection_rate']:.1f}%")
    content.append("")
    content.append(f"Temps de parole claire: {stats['total_clear_speech_formatted']} ({stats['clear_speech_percentage']:.1f}%)")
    content.append(f"Temps total d√©tect√©: {stats['total_detected_speech_formatted']} ({stats['detected_speech_percentage']:.1f}%)")
    content.append(f"Temps rejet√©: {stats['total_rejected_formatted']}")
    content.append("")
    content.append(f"Confiance moyenne (parole claire): {stats['average_confidence_clear']:.1f}%")
    content.append(f"Confiance moyenne (tous segments): {stats['average_confidence_all']:.1f}%")
    
    # Qualit√© des segments
    quality = stats['quality_metrics']
    content.append("")
    content.append("QUALIT√â DES SEGMENTS DE PAROLE CLAIRE:")
    content.append(f"Haute confiance (‚â•80%): {quality['segments_with_high_confidence']}")
    content.append(f"Confiance moyenne (60-79%): {quality['segments_with_medium_confidence']}")
    content.append(f"Faible confiance (<60%): {quality['segments_with_low_confidence']}")
    
    content.append("\n" + "="*70 + "\n")
    
    # Segments de parole claire
    content.append("SEGMENTS DE PAROLE CLAIRE VALID√âS:\n")
    
    for segment in analysis_result['clear_speech_segments']:
        confidence_icon = "üü¢" if segment['confidence_score'] >= 80 else "üü°" if segment['confidence_score'] >= 60 else "üü†"
        content.append(
            f"{segment['id']:3d}. {confidence_icon} PAROLE CLAIRE | "
            f"{segment['start_formatted']} ‚Üí {segment['end_formatted']} | "
            f"Dur√©e: {segment['duration_formatted']} | "
            f"Confiance: {segment['confidence_score']:.1f}%"
        )
    
    # Segments rejet√©s (optionnel, pour debug)
    rejected = [s for s in analysis_result['all_detected_segments'] if not s['is_clear_speech']]
    if rejected:
        content.append(f"\n" + "="*70)
        content.append("SEGMENTS REJET√âS (probables murmures/bruits):\n")
        
        for i, segment in enumerate(rejected, 1):
            content.append(
                f"{i:3d}. üî¥ REJET√â        | "
                f"{segment['start_formatted']} ‚Üí {segment['end_formatted']} | "
                f"Dur√©e: {segment['duration_formatted']} | "
                f"Confiance: {segment['confidence_score']:.1f}%"
            )
            
            # Ajouter quelques raisons du rejet
            main_reasons = [r for r in segment['analysis_reasons'] if r.startswith('‚úó')][:3]
            if main_reasons:
                content.append(f"     Raisons: {'; '.join(main_reasons)}")
            content.append("")
    
    # Sauvegarder
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(content))
    
    return output_path

def main():
    parser = argparse.ArgumentParser(
        description='Analyse audio pour d√©tecter UNIQUEMENT la parole claire (pas les murmures)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  %(prog)s audio.mp3                              # Analyse normale avec filtrage acoustique
  %(prog)s audio.mp3 --strict                     # Mode strict (filtrage s√©v√®re)
  %(prog)s audio.mp3 --format text --pretty       # Rapport d√©taill√© au format texte
  %(prog)s audio.mp3 --threshold 0.7 --strict     # Seuil √©lev√© + mode strict
  %(prog)s audio.mp3 --debug                      # Inclut les segments rejet√©s dans l'analyse
        """
    )
    
    parser.add_argument('audio_file', help='Fichier audio √† analyser')
    parser.add_argument('-o', '--output', help='Fichier de sortie (d√©faut: [audio_file]_clear_vad.json)')
    parser.add_argument('-f', '--format', choices=['json', 'text', 'both'], 
                       default='json', help='Format de sortie (d√©faut: json)')
    parser.add_argument('--threshold', type=float, default=0.5, 
                       help='Seuil de d√©tection VAD (0.1-0.9, d√©faut: 0.5)')
    parser.add_argument('--min-speech', type=int, default=250,
                       help='Dur√©e minimale de parole en ms (d√©faut: 250)')
    parser.add_argument('--min-silence', type=int, default=100,
                       help='Dur√©e minimale de silence en ms (d√©faut: 100)')
    parser.add_argument('--speech-pad', type=int, default=30,
                       help='Padding autour de la parole en ms (d√©faut: 30)')
    parser.add_argument('--strict', action='store_true',
                       help='Mode strict: filtrage s√©v√®re des murmures/chuchotements')
    parser.add_argument('--debug', action='store_true',
                       help='Inclut les segments rejet√©s dans l\'analyse')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='Mode silencieux (pas de sortie console)')
    parser.add_argument('--pretty', action='store_true',
                       help='Formatage JSON indent√©')
    
    args = parser.parse_args()
    
    try:
        # V√©rifier le fichier d'entr√©e
        if not os.path.exists(args.audio_file):
            print(f"‚ùå Erreur: Fichier introuvable: {args.audio_file}", file=sys.stderr)
            sys.exit(1)
        
        # Configuration VAD adapt√©e au mode strict
        config = {
            'threshold': args.threshold,
            'min_speech_duration_ms': args.min_speech,
            'min_silence_duration_ms': args.min_silence,
            'window_size_samples': 1536,
            'speech_pad_ms': args.speech_pad
        }
        
        # Initialiser l'analyseur
        if not args.quiet:
            mode_text = "STRICT" if args.strict else "NORMAL"
            print(f"üîÑ Chargement du mod√®le VAD avanc√© (mode {mode_text})...")
        
        analyzer = AudioVADAnalyzer()
        
        # Analyser le fichier
        if not args.quiet:
            print(f"üîÑ Analyse acoustique avanc√©e: {args.audio_file}")
            print("üîç Filtrage des murmures et chuchotements...")
        
        result = analyzer.analyze_audio(args.audio_file, config, strict_mode=args.strict)
        
        # D√©terminer les fichiers de sortie
        base_name = Path(args.audio_file).stem
        
        if args.output:
            output_base = Path(args.output).stem
            output_dir = Path(args.output).parent
        else:
            suffix = "_clear_strict_vad" if args.strict else "_clear_vad"
            output_base = f"{base_name}{suffix}"
            output_dir = Path.cwd()
        
        # Nettoyer le r√©sultat si pas en mode debug
        if not args.debug:
            # Supprimer les d√©tails des segments rejet√©s pour all√©ger la sortie
            result['all_detected_segments'] = [
                {k: v for k, v in seg.items() if k not in ['acoustic_features', 'analysis_reasons']}
                for seg in result['all_detected_segments']
            ]
        
        # Sauvegarder selon le format demand√©
        if args.format in ['json', 'both']:
            json_file = output_dir / f"{output_base}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                if args.pretty:
                    json.dump(result, f, indent=2, ensure_ascii=False)
                else:
                    json.dump(result, f, ensure_ascii=False)
            
            if not args.quiet:
                print(f"‚úÖ R√©sultats JSON sauvegard√©s: {json_file}")
        
        if args.format in ['text', 'both']:
            text_file = output_dir / f"{output_base}.txt"
            create_enhanced_text_report(result, text_file)
            
            if not args.quiet:
                print(f"‚úÖ Rapport d√©taill√© sauvegard√©: {text_file}")
        
        # Afficher un r√©sum√© si pas en mode silencieux
        if not args.quiet:
            stats = result['statistics']
            print(f"\nüìä R√©sum√© de l'analyse (mode {stats['analysis_mode'].upper()}):")
            print(f"   Dur√©e totale: {result['file_info']['total_duration_formatted']}")
            print(f"   Segments d√©tect√©s: {stats['total_segments_detected']}")
            print(f"   Parole claire: {stats['clear_speech_segments']} segments")
            print(f"   Segments rejet√©s: {stats['rejected_segments']} (taux: {stats['rejection_rate']:.1f}%)")
            print(f"   Temps de parole claire: {stats['total_clear_speech_formatted']} ({stats['clear_speech_percentage']:.1f}%)")
            print(f"   Confiance moyenne: {stats['average_confidence_clear']:.1f}%")
            
            if stats['clear_speech_segments'] == 0:
                print(f"   ‚ö†Ô∏è  Aucune parole claire d√©tect√©e - essayez le mode normal ou v√©rifiez l'audio")
            elif stats['rejection_rate'] > 50:
                print(f"   üí° Taux de rejet √©lev√© - l'audio contient beaucoup de murmures/bruits")
    
    except KeyboardInterrupt:
        print(f"\n‚ùå Analyse interrompue par l'utilisateur", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Erreur: {e}", file=sys.stderr)
        if not args.quiet:
            print(f"\nüí° V√©rifiez que:", file=sys.stderr)
            print(f"   1. Le fichier audio existe et est accessible", file=sys.stderr)
            print(f"   2. Les d√©pendances sont install√©es:", file=sys.stderr)
            print(f"      pip install torch torchaudio librosa scikit-learn scipy", file=sys.stderr)
            print(f"   3. Le format audio est support√© (.wav, .mp3, .m4a, .flac, etc.)", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()