import torch
import torchaudio
import numpy as np
from datetime import timedelta
import os

def format_time(seconds):
    """Convertit les secondes en format HH:MM:SS.mmm"""
    td = timedelta(seconds=seconds)
    total_seconds = int(td.total_seconds())
    hours, remainder = divmod(total_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = int((td.total_seconds() - total_seconds) * 1000)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

def detect_speech_segments(audio_path, output_txt_path=None):
    """
    D√©tecte les segments de parole et de silence dans un fichier audio
    
    Args:
        audio_path: Chemin vers le fichier audio
        output_txt_path: Chemin de sortie pour le fichier texte (optionnel)
    """
    
    # Charger le mod√®le Silero VAD
    model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    # Lire le fichier audio
    print(f"Lecture du fichier: {audio_path}")
    wav = read_audio(audio_path, sampling_rate=16000)
    
    # D√©tecter les timestamps de parole
    speech_timestamps = get_speech_timestamps(
        wav, 
        model, 
        sampling_rate=16000,
        threshold=0.5,          # Seuil de d√©tection (0.1-0.9)
        min_speech_duration_ms=250,  # Dur√©e minimale de parole (ms)
        min_silence_duration_ms=100, # Dur√©e minimale de silence (ms)
        window_size_samples=1536,    # Taille de fen√™tre
        speech_pad_ms=30            # Padding autour de la parole (ms)
    )
    
    # Pr√©parer les r√©sultats
    results = []
    total_duration = len(wav) / 16000  # Dur√©e totale en secondes
    
    # Ajouter un silence initial si n√©cessaire
    if speech_timestamps and speech_timestamps[0]['start'] > 0:
        silence_duration = speech_timestamps[0]['start'] / 16000
        results.append({
            'type': 'SILENCE',
            'start': 0,
            'end': silence_duration,
            'duration': silence_duration
        })
    
    # Traiter chaque segment de parole et les silences entre eux
    for i, segment in enumerate(speech_timestamps):
        start_time = segment['start'] / 16000
        end_time = segment['end'] / 16000
        duration = end_time - start_time
        
        # Ajouter le segment de parole
        results.append({
            'type': 'PAROLE',
            'start': start_time,
            'end': end_time,
            'duration': duration
        })
        
        # Ajouter le silence apr√®s ce segment (s'il y en a un)
        if i < len(speech_timestamps) - 1:
            next_start = speech_timestamps[i + 1]['start'] / 16000
            if next_start > end_time:
                silence_duration = next_start - end_time
                results.append({
                    'type': 'SILENCE',
                    'start': end_time,
                    'end': next_start,
                    'duration': silence_duration
                })
    
    # Ajouter un silence final si n√©cessaire
    if speech_timestamps and speech_timestamps[-1]['end'] / 16000 < total_duration:
        final_silence_start = speech_timestamps[-1]['end'] / 16000
        silence_duration = total_duration - final_silence_start
        results.append({
            'type': 'SILENCE',
            'start': final_silence_start,
            'end': total_duration,
            'duration': silence_duration
        })
    
    # G√©n√©rer le contenu texte
    txt_content = []
    txt_content.append("=== ANALYSE AUDIO - D√âTECTION PAROLE/SILENCE ===\n")
    txt_content.append(f"Fichier: {os.path.basename(audio_path)}")
    txt_content.append(f"Dur√©e totale: {format_time(total_duration)}")
    txt_content.append(f"Segments d√©tect√©s: {len(results)}")
    txt_content.append(f"Segments de parole: {len([r for r in results if r['type'] == 'PAROLE'])}")
    txt_content.append(f"Segments de silence: {len([r for r in results if r['type'] == 'SILENCE'])}")
    txt_content.append("\n" + "="*60 + "\n")
    
    # Statistiques
    total_speech = sum(r['duration'] for r in results if r['type'] == 'PAROLE')
    total_silence = sum(r['duration'] for r in results if r['type'] == 'SILENCE')
    
    txt_content.append("STATISTIQUES:")
    txt_content.append(f"Temps total de parole: {format_time(total_speech)} ({total_speech/total_duration*100:.1f}%)")
    txt_content.append(f"Temps total de silence: {format_time(total_silence)} ({total_silence/total_duration*100:.1f}%)")
    txt_content.append("\n" + "="*60 + "\n")
    
    # D√©tails des segments
    txt_content.append("SEGMENTS D√âTAILL√âS:\n")
    
    for i, segment in enumerate(results, 1):
        start_formatted = format_time(segment['start'])
        end_formatted = format_time(segment['end'])
        duration_formatted = format_time(segment['duration'])
        
        if segment['type'] == 'PAROLE':
            txt_content.append(f"{i:3d}. üé§ PAROLE    | {start_formatted} ‚Üí {end_formatted} | Dur√©e: {duration_formatted}")
        else:
            txt_content.append(f"{i:3d}. üîá SILENCE   | {start_formatted} ‚Üí {end_formatted} | Dur√©e: {duration_formatted}")
    
    # Sauvegarder le fichier texte
    if output_txt_path is None:
        output_txt_path = audio_path.rsplit('.', 1)[0] + '_vad_analysis.txt'
    
    with open(output_txt_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(txt_content))
    
    print(f"\nAnalyse termin√©e!")
    print(f"R√©sultats sauvegard√©s dans: {output_txt_path}")
    print(f"Segments d√©tect√©s: {len(results)}")
    print(f"Parole: {format_time(total_speech)} | Silence: {format_time(total_silence)}")
    
    return results, output_txt_path

def analyze_short_silences(results, max_silence_duration=2.0):
    """
    Analyse sp√©cifique des courts silences (respirations, pauses)
    
    Args:
        results: R√©sultats de l'analyse VAD
        max_silence_duration: Dur√©e max pour consid√©rer comme "court silence"
    """
    short_silences = [
        r for r in results 
        if r['type'] == 'SILENCE' and r['duration'] <= max_silence_duration
    ]
    
    print(f"\n=== ANALYSE DES COURTS SILENCES (‚â§ {max_silence_duration}s) ===")
    print(f"Nombre de courts silences: {len(short_silences)}")
    
    if short_silences:
        avg_duration = np.mean([s['duration'] for s in short_silences])
        print(f"Dur√©e moyenne: {avg_duration:.3f}s")
        
        print("\nCourts silences d√©tect√©s:")
        for silence in short_silences:
            start_formatted = format_time(silence['start'])
            duration_formatted = f"{silence['duration']:.3f}s"
            print(f"  ‚Ä¢ {start_formatted} - {duration_formatted} (respiration/pause)")

# Exemple d'utilisation
if __name__ == "__main__":
    # Remplacez par le chemin de votre fichier audio
    audio_file = "./audios/tabac.mp3"  # ou .wav, .m4a, etc.
    
    try:
        # Analyser le fichier
        segments, output_file = detect_speech_segments(audio_file)
        
        # Analyser les courts silences (respirations, pauses)
        analyze_short_silences(segments, max_silence_duration=1.5)
        
        print(f"\n‚úÖ Fichier d'analyse cr√©√©: {output_file}")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        print("\nV√©rifiez que:")
        print("1. Le fichier audio existe")
        print("2. torch et torchaudio sont install√©s: pip install torch torchaudio")
        print("3. Le format audio est support√© (.wav, .mp3, .m4a, .flac, etc.)")